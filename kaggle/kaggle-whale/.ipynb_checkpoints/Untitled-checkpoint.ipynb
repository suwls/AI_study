{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to import some libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "# print(os.listdir(\"../input\"))\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000e88ab.jpg</td>\n",
       "      <td>w_f48451c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001f9222.jpg</td>\n",
       "      <td>w_c3d896a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00029d126.jpg</td>\n",
       "      <td>w_20df2c5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00050a15a.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0005c1ef8.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Image         Id\n",
       "0  0000e88ab.jpg  w_f48451c\n",
       "1  0001f9222.jpg  w_c3d896a\n",
       "2  00029d126.jpg  w_20df2c5\n",
       "3  00050a15a.jpg  new_whale\n",
       "4  0005c1ef8.jpg  new_whale"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load image data\n",
    "dataFile = pd.read_csv('./input/train.csv')\n",
    "dataFile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhaleDataset(Dataset):\n",
    "    def __init__(self, datafolder, datatype='train', dataFile=None, transform=None, labelArray=None):\n",
    "        self.datafolder = datafolder\n",
    "        self.datatype = datatype\n",
    "        self.labelArray = labelArray\n",
    "        if self.datatype == 'train':\n",
    "            self.dataFile = dataFile.values\n",
    "        self.image_files_list = [s for s in os.listdir(datafolder)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.datatype == 'train':\n",
    "            img_name = os.path.join(self.datafolder, self.dataFile[idx][0])\n",
    "            label = self.labelArray[idx]\n",
    "            \n",
    "        elif self.datatype == 'test':\n",
    "            img_name = os.path.join(self.datafolder, self.image_files_list[idx])\n",
    "            label = np.zeros((5005,))\n",
    "        \n",
    "        img = Image.open(img_name).convert('RGB')\n",
    "        image = self.transform(img)\n",
    "        \n",
    "        if self.datatype == 'train':\n",
    "            return image, label\n",
    "        elif self.datatype == 'test':\n",
    "            # so that the images will be in a correct order\n",
    "            return image, label, self.image_files_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def prepare_labels(y):\n",
    "    values = np.array(y)\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    #print(onehot_encoded.shape)\n",
    "\n",
    "    y = onehot_encoded\n",
    "    #print(y.shape)\n",
    "    return y, label_encoder\n",
    "\n",
    "y, label_encoder = prepare_labels(dataFile['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained = True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 5005)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25361\n"
     ]
    }
   ],
   "source": [
    "input_size = 224 \n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "data_transforms =  transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),  # simple data augmentation\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "        ])\n",
    "train_dataset= WhaleDataset(datafolder='./input/train/', datatype='train', \n",
    "                            dataFile=dataFile, transform=data_transforms, \n",
    "                            labelArray=y)\n",
    "dset_loaders = torch.utils.data.DataLoader(train_dataset, batch_size=32, num_workers=0, pin_memory=True)\n",
    "N_train = len(y)\n",
    "print(N_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "| Epoch [ 1/ 5] Iter [  1/396]\tBatch loss 0.0110\n",
      "| Epoch [ 1/ 5] Iter [ 11/396]\tBatch loss 0.0110\n",
      "| Epoch [ 1/ 5] Iter [ 21/396]\tBatch loss 0.0110\n",
      "| Epoch [ 1/ 5] Iter [ 31/396]\tBatch loss 0.0110\n",
      "| Epoch [ 1/ 5] Iter [ 41/396]\tBatch loss 0.0109\n",
      "| Epoch [ 1/ 5] Iter [ 51/396]\tBatch loss 0.0109\n",
      "| Epoch [ 1/ 5] Iter [ 61/396]\tBatch loss 0.0109\n",
      "| Epoch [ 1/ 5] Iter [ 71/396]\tBatch loss 0.0108\n",
      "| Epoch [ 1/ 5] Iter [ 81/396]\tBatch loss 0.0108\n",
      "| Epoch [ 1/ 5] Iter [ 91/396]\tBatch loss 0.0108\n",
      "| Epoch [ 1/ 5] Iter [101/396]\tBatch loss 0.0107\n",
      "| Epoch [ 1/ 5] Iter [111/396]\tBatch loss 0.0107\n",
      "| Epoch [ 1/ 5] Iter [121/396]\tBatch loss 0.0107\n",
      "| Epoch [ 1/ 5] Iter [131/396]\tBatch loss 0.0106\n",
      "| Epoch [ 1/ 5] Iter [141/396]\tBatch loss 0.0106\n",
      "| Epoch [ 1/ 5] Iter [151/396]\tBatch loss 0.0106\n",
      "| Epoch [ 1/ 5] Iter [161/396]\tBatch loss 0.0105\n",
      "| Epoch [ 1/ 5] Iter [171/396]\tBatch loss 0.0105\n",
      "| Epoch [ 1/ 5] Iter [181/396]\tBatch loss 0.0105\n",
      "| Epoch [ 1/ 5] Iter [191/396]\tBatch loss 0.0104\n",
      "| Epoch [ 1/ 5] Iter [201/396]\tBatch loss 0.0104\n",
      "| Epoch [ 1/ 5] Iter [211/396]\tBatch loss 0.0104\n",
      "| Epoch [ 1/ 5] Iter [221/396]\tBatch loss 0.0103\n",
      "| Epoch [ 1/ 5] Iter [231/396]\tBatch loss 0.0103\n",
      "| Epoch [ 1/ 5] Iter [241/396]\tBatch loss 0.0103\n",
      "| Epoch [ 1/ 5] Iter [251/396]\tBatch loss 0.0102\n",
      "| Epoch [ 1/ 5] Iter [261/396]\tBatch loss 0.0102\n",
      "| Epoch [ 1/ 5] Iter [271/396]\tBatch loss 0.0102\n",
      "| Epoch [ 1/ 5] Iter [281/396]\tBatch loss 0.0101\n",
      "| Epoch [ 1/ 5] Iter [291/396]\tBatch loss 0.0101\n",
      "| Epoch [ 1/ 5] Iter [301/396]\tBatch loss 0.0101\n",
      "| Epoch [ 1/ 5] Iter [311/396]\tBatch loss 0.0101\n",
      "| Epoch [ 1/ 5] Iter [321/396]\tBatch loss 0.0100\n",
      "| Epoch [ 1/ 5] Iter [331/396]\tBatch loss 0.0100\n",
      "| Epoch [ 1/ 5] Iter [341/396]\tBatch loss 0.0100\n",
      "| Epoch [ 1/ 5] Iter [351/396]\tBatch loss 0.0099\n",
      "| Epoch [ 1/ 5] Iter [361/396]\tBatch loss 0.0099\n",
      "| Epoch [ 1/ 5] Iter [371/396]\tBatch loss 0.0099\n",
      "| Epoch [ 1/ 5] Iter [381/396]\tBatch loss 0.0098\n",
      "| Epoch [ 1/ 5] Iter [391/396]\tBatch loss 0.0098\n",
      "| Epoch [ 1/ 5] Iter [401/396]\tBatch loss 0.0098\n",
      "| Epoch [ 1/ 5] Iter [411/396]\tBatch loss 0.0098\n",
      "| Epoch [ 1/ 5] Iter [421/396]\tBatch loss 0.0097\n",
      "| Epoch [ 1/ 5] Iter [431/396]\tBatch loss 0.0097\n",
      "| Epoch [ 1/ 5] Iter [441/396]\tBatch loss 0.0097\n",
      "| Epoch [ 1/ 5] Iter [451/396]\tBatch loss 0.0096\n",
      "| Epoch [ 1/ 5] Iter [461/396]\tBatch loss 0.0096\n",
      "| Epoch [ 1/ 5] Iter [471/396]\tBatch loss 0.0096\n",
      "| Epoch [ 1/ 5] Iter [481/396]\tBatch loss 0.0095\n",
      "| Epoch [ 1/ 5] Iter [491/396]\tBatch loss 0.0095\n",
      "| Epoch [ 1/ 5] Iter [501/396]\tBatch loss 0.0095\n",
      "| Epoch [ 1/ 5] Iter [511/396]\tBatch loss 0.0095\n",
      "| Epoch [ 1/ 5] Iter [521/396]\tBatch loss 0.0094\n",
      "| Epoch [ 1/ 5] Iter [531/396]\tBatch loss 0.0094\n",
      "| Epoch [ 1/ 5] Iter [541/396]\tBatch loss 0.0094\n",
      "| Epoch [ 1/ 5] Iter [551/396]\tBatch loss 0.0093\n",
      "| Epoch [ 1/ 5] Iter [561/396]\tBatch loss 0.0093\n",
      "| Epoch [ 1/ 5] Iter [571/396]\tBatch loss 0.0093\n",
      "| Epoch [ 1/ 5] Iter [581/396]\tBatch loss 0.0093\n",
      "| Epoch [ 1/ 5] Iter [591/396]\tBatch loss 0.0092\n",
      "| Epoch [ 1/ 5] Iter [601/396]\tBatch loss 0.0092\n",
      "| Epoch [ 1/ 5] Iter [611/396]\tBatch loss 0.0092\n",
      "| Epoch [ 1/ 5] Iter [621/396]\tBatch loss 0.0091\n",
      "| Epoch [ 1/ 5] Iter [631/396]\tBatch loss 0.0091\n",
      "| Epoch [ 1/ 5] Iter [641/396]\tBatch loss 0.0091\n",
      "| Epoch [ 1/ 5] Iter [651/396]\tBatch loss 0.0091\n",
      "| Epoch [ 1/ 5] Iter [661/396]\tBatch loss 0.0090\n",
      "| Epoch [ 1/ 5] Iter [671/396]\tBatch loss 0.0090\n",
      "| Epoch [ 1/ 5] Iter [681/396]\tBatch loss 0.0090\n",
      "| Epoch [ 1/ 5] Iter [691/396]\tBatch loss 0.0089\n",
      "| Epoch [ 1/ 5] Iter [701/396]\tBatch loss 0.0089\n",
      "| Epoch [ 1/ 5] Iter [711/396]\tBatch loss 0.0089\n",
      "| Epoch [ 1/ 5] Iter [721/396]\tBatch loss 0.0089\n",
      "| Epoch [ 1/ 5] Iter [731/396]\tBatch loss 0.0088\n",
      "| Epoch [ 1/ 5] Iter [741/396]\tBatch loss 0.0088\n",
      "| Epoch [ 1/ 5] Iter [751/396]\tBatch loss 0.0088\n",
      "| Epoch [ 1/ 5] Iter [761/396]\tBatch loss 0.0087\n",
      "| Epoch [ 1/ 5] Iter [771/396]\tBatch loss 0.0087\n",
      "| Epoch [ 1/ 5] Iter [781/396]\tBatch loss 0.0087\n",
      "| Epoch [ 1/ 5] Iter [791/396]\tBatch loss 0.0087\n",
      "\n",
      "| Training loss 0.6478\n",
      "Epoch 1/4\n",
      "----------\n",
      "| Epoch [ 2/ 5] Iter [  1/396]\tBatch loss 0.0087\n",
      "| Epoch [ 2/ 5] Iter [ 11/396]\tBatch loss 0.0086\n",
      "| Epoch [ 2/ 5] Iter [ 21/396]\tBatch loss 0.0086\n",
      "| Epoch [ 2/ 5] Iter [ 31/396]\tBatch loss 0.0086\n",
      "| Epoch [ 2/ 5] Iter [ 41/396]\tBatch loss 0.0086\n",
      "| Epoch [ 2/ 5] Iter [ 51/396]\tBatch loss 0.0085\n",
      "| Epoch [ 2/ 5] Iter [ 61/396]\tBatch loss 0.0085\n",
      "| Epoch [ 2/ 5] Iter [ 71/396]\tBatch loss 0.0085\n",
      "| Epoch [ 2/ 5] Iter [ 81/396]\tBatch loss 0.0085\n",
      "| Epoch [ 2/ 5] Iter [ 91/396]\tBatch loss 0.0084\n",
      "| Epoch [ 2/ 5] Iter [101/396]\tBatch loss 0.0084\n",
      "| Epoch [ 2/ 5] Iter [111/396]\tBatch loss 0.0084\n",
      "| Epoch [ 2/ 5] Iter [121/396]\tBatch loss 0.0084\n",
      "| Epoch [ 2/ 5] Iter [131/396]\tBatch loss 0.0083\n",
      "| Epoch [ 2/ 5] Iter [141/396]\tBatch loss 0.0083\n",
      "| Epoch [ 2/ 5] Iter [151/396]\tBatch loss 0.0083\n",
      "| Epoch [ 2/ 5] Iter [161/396]\tBatch loss 0.0083\n",
      "| Epoch [ 2/ 5] Iter [171/396]\tBatch loss 0.0082\n",
      "| Epoch [ 2/ 5] Iter [181/396]\tBatch loss 0.0082\n",
      "| Epoch [ 2/ 5] Iter [191/396]\tBatch loss 0.0082\n",
      "| Epoch [ 2/ 5] Iter [201/396]\tBatch loss 0.0082\n",
      "| Epoch [ 2/ 5] Iter [211/396]\tBatch loss 0.0081\n",
      "| Epoch [ 2/ 5] Iter [221/396]\tBatch loss 0.0081\n",
      "| Epoch [ 2/ 5] Iter [231/396]\tBatch loss 0.0081\n",
      "| Epoch [ 2/ 5] Iter [241/396]\tBatch loss 0.0081\n",
      "| Epoch [ 2/ 5] Iter [251/396]\tBatch loss 0.0080\n",
      "| Epoch [ 2/ 5] Iter [261/396]\tBatch loss 0.0080\n",
      "| Epoch [ 2/ 5] Iter [271/396]\tBatch loss 0.0080\n",
      "| Epoch [ 2/ 5] Iter [281/396]\tBatch loss 0.0080\n",
      "| Epoch [ 2/ 5] Iter [291/396]\tBatch loss 0.0079\n",
      "| Epoch [ 2/ 5] Iter [301/396]\tBatch loss 0.0079\n",
      "| Epoch [ 2/ 5] Iter [311/396]\tBatch loss 0.0079\n",
      "| Epoch [ 2/ 5] Iter [321/396]\tBatch loss 0.0079\n",
      "| Epoch [ 2/ 5] Iter [331/396]\tBatch loss 0.0078\n",
      "| Epoch [ 2/ 5] Iter [341/396]\tBatch loss 0.0078\n",
      "| Epoch [ 2/ 5] Iter [351/396]\tBatch loss 0.0078\n",
      "| Epoch [ 2/ 5] Iter [361/396]\tBatch loss 0.0078\n",
      "| Epoch [ 2/ 5] Iter [371/396]\tBatch loss 0.0077\n",
      "| Epoch [ 2/ 5] Iter [381/396]\tBatch loss 0.0077\n",
      "| Epoch [ 2/ 5] Iter [391/396]\tBatch loss 0.0077\n",
      "| Epoch [ 2/ 5] Iter [401/396]\tBatch loss 0.0077\n",
      "| Epoch [ 2/ 5] Iter [411/396]\tBatch loss 0.0077\n",
      "| Epoch [ 2/ 5] Iter [421/396]\tBatch loss 0.0076\n",
      "| Epoch [ 2/ 5] Iter [431/396]\tBatch loss 0.0076\n",
      "| Epoch [ 2/ 5] Iter [441/396]\tBatch loss 0.0076\n",
      "| Epoch [ 2/ 5] Iter [451/396]\tBatch loss 0.0076\n",
      "| Epoch [ 2/ 5] Iter [461/396]\tBatch loss 0.0075\n",
      "| Epoch [ 2/ 5] Iter [471/396]\tBatch loss 0.0075\n",
      "| Epoch [ 2/ 5] Iter [481/396]\tBatch loss 0.0075\n",
      "| Epoch [ 2/ 5] Iter [491/396]\tBatch loss 0.0075\n",
      "| Epoch [ 2/ 5] Iter [501/396]\tBatch loss 0.0075\n",
      "| Epoch [ 2/ 5] Iter [511/396]\tBatch loss 0.0074\n",
      "| Epoch [ 2/ 5] Iter [521/396]\tBatch loss 0.0074\n",
      "| Epoch [ 2/ 5] Iter [531/396]\tBatch loss 0.0074\n",
      "| Epoch [ 2/ 5] Iter [541/396]\tBatch loss 0.0074\n",
      "| Epoch [ 2/ 5] Iter [551/396]\tBatch loss 0.0074\n",
      "| Epoch [ 2/ 5] Iter [561/396]\tBatch loss 0.0073\n",
      "| Epoch [ 2/ 5] Iter [571/396]\tBatch loss 0.0073\n",
      "| Epoch [ 2/ 5] Iter [581/396]\tBatch loss 0.0073\n",
      "| Epoch [ 2/ 5] Iter [591/396]\tBatch loss 0.0073\n",
      "| Epoch [ 2/ 5] Iter [601/396]\tBatch loss 0.0072\n",
      "| Epoch [ 2/ 5] Iter [611/396]\tBatch loss 0.0072\n",
      "| Epoch [ 2/ 5] Iter [621/396]\tBatch loss 0.0072\n",
      "| Epoch [ 2/ 5] Iter [631/396]\tBatch loss 0.0072\n",
      "| Epoch [ 2/ 5] Iter [641/396]\tBatch loss 0.0072\n",
      "| Epoch [ 2/ 5] Iter [651/396]\tBatch loss 0.0071\n",
      "| Epoch [ 2/ 5] Iter [661/396]\tBatch loss 0.0071\n",
      "| Epoch [ 2/ 5] Iter [671/396]\tBatch loss 0.0071\n",
      "| Epoch [ 2/ 5] Iter [681/396]\tBatch loss 0.0071\n",
      "| Epoch [ 2/ 5] Iter [691/396]\tBatch loss 0.0070\n",
      "| Epoch [ 2/ 5] Iter [701/396]\tBatch loss 0.0070\n",
      "| Epoch [ 2/ 5] Iter [711/396]\tBatch loss 0.0070\n",
      "| Epoch [ 2/ 5] Iter [721/396]\tBatch loss 0.0070\n",
      "| Epoch [ 2/ 5] Iter [731/396]\tBatch loss 0.0070\n",
      "| Epoch [ 2/ 5] Iter [741/396]\tBatch loss 0.0070\n",
      "| Epoch [ 2/ 5] Iter [751/396]\tBatch loss 0.0069\n",
      "| Epoch [ 2/ 5] Iter [761/396]\tBatch loss 0.0069\n",
      "| Epoch [ 2/ 5] Iter [771/396]\tBatch loss 0.0069\n",
      "| Epoch [ 2/ 5] Iter [781/396]\tBatch loss 0.0069\n",
      "| Epoch [ 2/ 5] Iter [791/396]\tBatch loss 0.0069\n",
      "\n",
      "| Training loss 0.5091\n",
      "Epoch 2/4\n",
      "----------\n",
      "| Epoch [ 3/ 5] Iter [  1/396]\tBatch loss 0.0068\n",
      "| Epoch [ 3/ 5] Iter [ 11/396]\tBatch loss 0.0068\n",
      "| Epoch [ 3/ 5] Iter [ 21/396]\tBatch loss 0.0068\n",
      "| Epoch [ 3/ 5] Iter [ 31/396]\tBatch loss 0.0068\n",
      "| Epoch [ 3/ 5] Iter [ 41/396]\tBatch loss 0.0068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch [ 3/ 5] Iter [ 51/396]\tBatch loss 0.0068\n",
      "| Epoch [ 3/ 5] Iter [ 61/396]\tBatch loss 0.0067\n",
      "| Epoch [ 3/ 5] Iter [ 71/396]\tBatch loss 0.0067\n",
      "| Epoch [ 3/ 5] Iter [ 81/396]\tBatch loss 0.0067\n",
      "| Epoch [ 3/ 5] Iter [ 91/396]\tBatch loss 0.0067\n",
      "| Epoch [ 3/ 5] Iter [101/396]\tBatch loss 0.0067\n",
      "| Epoch [ 3/ 5] Iter [111/396]\tBatch loss 0.0066\n",
      "| Epoch [ 3/ 5] Iter [121/396]\tBatch loss 0.0066\n",
      "| Epoch [ 3/ 5] Iter [131/396]\tBatch loss 0.0066\n",
      "| Epoch [ 3/ 5] Iter [141/396]\tBatch loss 0.0066\n",
      "| Epoch [ 3/ 5] Iter [151/396]\tBatch loss 0.0066\n",
      "| Epoch [ 3/ 5] Iter [161/396]\tBatch loss 0.0065\n",
      "| Epoch [ 3/ 5] Iter [171/396]\tBatch loss 0.0065\n",
      "| Epoch [ 3/ 5] Iter [181/396]\tBatch loss 0.0065\n",
      "| Epoch [ 3/ 5] Iter [191/396]\tBatch loss 0.0065\n",
      "| Epoch [ 3/ 5] Iter [201/396]\tBatch loss 0.0065\n",
      "| Epoch [ 3/ 5] Iter [211/396]\tBatch loss 0.0065\n",
      "| Epoch [ 3/ 5] Iter [221/396]\tBatch loss 0.0064\n",
      "| Epoch [ 3/ 5] Iter [231/396]\tBatch loss 0.0064\n",
      "| Epoch [ 3/ 5] Iter [241/396]\tBatch loss 0.0064\n",
      "| Epoch [ 3/ 5] Iter [251/396]\tBatch loss 0.0064\n",
      "| Epoch [ 3/ 5] Iter [261/396]\tBatch loss 0.0064\n",
      "| Epoch [ 3/ 5] Iter [271/396]\tBatch loss 0.0064\n",
      "| Epoch [ 3/ 5] Iter [281/396]\tBatch loss 0.0063\n",
      "| Epoch [ 3/ 5] Iter [291/396]\tBatch loss 0.0063\n",
      "| Epoch [ 3/ 5] Iter [301/396]\tBatch loss 0.0063\n",
      "| Epoch [ 3/ 5] Iter [311/396]\tBatch loss 0.0063\n",
      "| Epoch [ 3/ 5] Iter [321/396]\tBatch loss 0.0062\n",
      "| Epoch [ 3/ 5] Iter [331/396]\tBatch loss 0.0062\n",
      "| Epoch [ 3/ 5] Iter [341/396]\tBatch loss 0.0062\n",
      "| Epoch [ 3/ 5] Iter [351/396]\tBatch loss 0.0062\n",
      "| Epoch [ 3/ 5] Iter [361/396]\tBatch loss 0.0062\n",
      "| Epoch [ 3/ 5] Iter [371/396]\tBatch loss 0.0062\n",
      "| Epoch [ 3/ 5] Iter [381/396]\tBatch loss 0.0062\n",
      "| Epoch [ 3/ 5] Iter [391/396]\tBatch loss 0.0061\n",
      "| Epoch [ 3/ 5] Iter [401/396]\tBatch loss 0.0061\n",
      "| Epoch [ 3/ 5] Iter [411/396]\tBatch loss 0.0061\n",
      "| Epoch [ 3/ 5] Iter [421/396]\tBatch loss 0.0061\n",
      "| Epoch [ 3/ 5] Iter [431/396]\tBatch loss 0.0061\n",
      "| Epoch [ 3/ 5] Iter [441/396]\tBatch loss 0.0061\n",
      "| Epoch [ 3/ 5] Iter [451/396]\tBatch loss 0.0060\n",
      "| Epoch [ 3/ 5] Iter [461/396]\tBatch loss 0.0060\n",
      "| Epoch [ 3/ 5] Iter [471/396]\tBatch loss 0.0060\n",
      "| Epoch [ 3/ 5] Iter [481/396]\tBatch loss 0.0060\n",
      "| Epoch [ 3/ 5] Iter [491/396]\tBatch loss 0.0060\n",
      "| Epoch [ 3/ 5] Iter [501/396]\tBatch loss 0.0060\n",
      "| Epoch [ 3/ 5] Iter [511/396]\tBatch loss 0.0059\n",
      "| Epoch [ 3/ 5] Iter [521/396]\tBatch loss 0.0059\n",
      "| Epoch [ 3/ 5] Iter [531/396]\tBatch loss 0.0059\n",
      "| Epoch [ 3/ 5] Iter [541/396]\tBatch loss 0.0059\n",
      "| Epoch [ 3/ 5] Iter [551/396]\tBatch loss 0.0059\n",
      "| Epoch [ 3/ 5] Iter [561/396]\tBatch loss 0.0059\n",
      "| Epoch [ 3/ 5] Iter [571/396]\tBatch loss 0.0058\n",
      "| Epoch [ 3/ 5] Iter [581/396]\tBatch loss 0.0058\n",
      "| Epoch [ 3/ 5] Iter [591/396]\tBatch loss 0.0058\n",
      "| Epoch [ 3/ 5] Iter [601/396]\tBatch loss 0.0058\n",
      "| Epoch [ 3/ 5] Iter [611/396]\tBatch loss 0.0058\n",
      "| Epoch [ 3/ 5] Iter [621/396]\tBatch loss 0.0058\n",
      "| Epoch [ 3/ 5] Iter [631/396]\tBatch loss 0.0057\n",
      "| Epoch [ 3/ 5] Iter [641/396]\tBatch loss 0.0057\n",
      "| Epoch [ 3/ 5] Iter [651/396]\tBatch loss 0.0057\n",
      "| Epoch [ 3/ 5] Iter [661/396]\tBatch loss 0.0057\n",
      "| Epoch [ 3/ 5] Iter [671/396]\tBatch loss 0.0057\n",
      "| Epoch [ 3/ 5] Iter [681/396]\tBatch loss 0.0057\n",
      "| Epoch [ 3/ 5] Iter [691/396]\tBatch loss 0.0057\n",
      "| Epoch [ 3/ 5] Iter [701/396]\tBatch loss 0.0056\n",
      "| Epoch [ 3/ 5] Iter [711/396]\tBatch loss 0.0056\n",
      "| Epoch [ 3/ 5] Iter [721/396]\tBatch loss 0.0056\n",
      "| Epoch [ 3/ 5] Iter [731/396]\tBatch loss 0.0056\n",
      "| Epoch [ 3/ 5] Iter [741/396]\tBatch loss 0.0056\n",
      "| Epoch [ 3/ 5] Iter [751/396]\tBatch loss 0.0056\n",
      "| Epoch [ 3/ 5] Iter [761/396]\tBatch loss 0.0056\n",
      "| Epoch [ 3/ 5] Iter [771/396]\tBatch loss 0.0055\n",
      "| Epoch [ 3/ 5] Iter [781/396]\tBatch loss 0.0055\n",
      "| Epoch [ 3/ 5] Iter [791/396]\tBatch loss 0.0055\n",
      "\n",
      "| Training loss 0.4056\n",
      "Epoch 3/4\n",
      "----------\n",
      "| Epoch [ 4/ 5] Iter [  1/396]\tBatch loss 0.0055\n",
      "| Epoch [ 4/ 5] Iter [ 11/396]\tBatch loss 0.0055\n",
      "| Epoch [ 4/ 5] Iter [ 21/396]\tBatch loss 0.0055\n",
      "| Epoch [ 4/ 5] Iter [ 31/396]\tBatch loss 0.0055\n",
      "| Epoch [ 4/ 5] Iter [ 41/396]\tBatch loss 0.0054\n",
      "| Epoch [ 4/ 5] Iter [ 51/396]\tBatch loss 0.0054\n",
      "| Epoch [ 4/ 5] Iter [ 61/396]\tBatch loss 0.0054\n",
      "| Epoch [ 4/ 5] Iter [ 71/396]\tBatch loss 0.0054\n",
      "| Epoch [ 4/ 5] Iter [ 81/396]\tBatch loss 0.0054\n",
      "| Epoch [ 4/ 5] Iter [ 91/396]\tBatch loss 0.0054\n",
      "| Epoch [ 4/ 5] Iter [101/396]\tBatch loss 0.0054\n",
      "| Epoch [ 4/ 5] Iter [111/396]\tBatch loss 0.0053\n",
      "| Epoch [ 4/ 5] Iter [121/396]\tBatch loss 0.0053\n",
      "| Epoch [ 4/ 5] Iter [131/396]\tBatch loss 0.0053\n",
      "| Epoch [ 4/ 5] Iter [141/396]\tBatch loss 0.0053\n",
      "| Epoch [ 4/ 5] Iter [151/396]\tBatch loss 0.0053\n",
      "| Epoch [ 4/ 5] Iter [161/396]\tBatch loss 0.0053\n",
      "| Epoch [ 4/ 5] Iter [171/396]\tBatch loss 0.0053\n",
      "| Epoch [ 4/ 5] Iter [181/396]\tBatch loss 0.0052\n",
      "| Epoch [ 4/ 5] Iter [191/396]\tBatch loss 0.0052\n",
      "| Epoch [ 4/ 5] Iter [201/396]\tBatch loss 0.0052\n",
      "| Epoch [ 4/ 5] Iter [211/396]\tBatch loss 0.0052\n",
      "| Epoch [ 4/ 5] Iter [221/396]\tBatch loss 0.0052\n",
      "| Epoch [ 4/ 5] Iter [231/396]\tBatch loss 0.0052\n",
      "| Epoch [ 4/ 5] Iter [241/396]\tBatch loss 0.0052\n",
      "| Epoch [ 4/ 5] Iter [251/396]\tBatch loss 0.0051\n",
      "| Epoch [ 4/ 5] Iter [261/396]\tBatch loss 0.0051\n",
      "| Epoch [ 4/ 5] Iter [271/396]\tBatch loss 0.0051\n",
      "| Epoch [ 4/ 5] Iter [281/396]\tBatch loss 0.0051\n",
      "| Epoch [ 4/ 5] Iter [291/396]\tBatch loss 0.0051\n",
      "| Epoch [ 4/ 5] Iter [301/396]\tBatch loss 0.0051\n",
      "| Epoch [ 4/ 5] Iter [311/396]\tBatch loss 0.0051\n",
      "| Epoch [ 4/ 5] Iter [321/396]\tBatch loss 0.0051\n",
      "| Epoch [ 4/ 5] Iter [331/396]\tBatch loss 0.0051\n",
      "| Epoch [ 4/ 5] Iter [341/396]\tBatch loss 0.0050\n",
      "| Epoch [ 4/ 5] Iter [351/396]\tBatch loss 0.0050\n",
      "| Epoch [ 4/ 5] Iter [361/396]\tBatch loss 0.0050\n",
      "| Epoch [ 4/ 5] Iter [371/396]\tBatch loss 0.0050\n",
      "| Epoch [ 4/ 5] Iter [381/396]\tBatch loss 0.0050\n",
      "| Epoch [ 4/ 5] Iter [391/396]\tBatch loss 0.0050\n",
      "| Epoch [ 4/ 5] Iter [401/396]\tBatch loss 0.0050\n",
      "| Epoch [ 4/ 5] Iter [411/396]\tBatch loss 0.0049\n",
      "| Epoch [ 4/ 5] Iter [421/396]\tBatch loss 0.0049\n",
      "| Epoch [ 4/ 5] Iter [431/396]\tBatch loss 0.0049\n",
      "| Epoch [ 4/ 5] Iter [441/396]\tBatch loss 0.0049\n",
      "| Epoch [ 4/ 5] Iter [451/396]\tBatch loss 0.0049\n",
      "| Epoch [ 4/ 5] Iter [461/396]\tBatch loss 0.0049\n",
      "| Epoch [ 4/ 5] Iter [471/396]\tBatch loss 0.0049\n",
      "| Epoch [ 4/ 5] Iter [481/396]\tBatch loss 0.0049\n",
      "| Epoch [ 4/ 5] Iter [491/396]\tBatch loss 0.0049\n",
      "| Epoch [ 4/ 5] Iter [501/396]\tBatch loss 0.0048\n",
      "| Epoch [ 4/ 5] Iter [511/396]\tBatch loss 0.0048\n",
      "| Epoch [ 4/ 5] Iter [521/396]\tBatch loss 0.0048\n",
      "| Epoch [ 4/ 5] Iter [531/396]\tBatch loss 0.0048\n",
      "| Epoch [ 4/ 5] Iter [541/396]\tBatch loss 0.0048\n",
      "| Epoch [ 4/ 5] Iter [551/396]\tBatch loss 0.0048\n",
      "| Epoch [ 4/ 5] Iter [561/396]\tBatch loss 0.0048\n",
      "| Epoch [ 4/ 5] Iter [571/396]\tBatch loss 0.0048\n",
      "| Epoch [ 4/ 5] Iter [581/396]\tBatch loss 0.0047\n",
      "| Epoch [ 4/ 5] Iter [591/396]\tBatch loss 0.0047\n",
      "| Epoch [ 4/ 5] Iter [601/396]\tBatch loss 0.0047\n",
      "| Epoch [ 4/ 5] Iter [611/396]\tBatch loss 0.0047\n",
      "| Epoch [ 4/ 5] Iter [621/396]\tBatch loss 0.0047\n",
      "| Epoch [ 4/ 5] Iter [631/396]\tBatch loss 0.0047\n",
      "| Epoch [ 4/ 5] Iter [641/396]\tBatch loss 0.0047\n",
      "| Epoch [ 4/ 5] Iter [651/396]\tBatch loss 0.0047\n",
      "| Epoch [ 4/ 5] Iter [661/396]\tBatch loss 0.0046\n",
      "| Epoch [ 4/ 5] Iter [671/396]\tBatch loss 0.0046\n",
      "| Epoch [ 4/ 5] Iter [681/396]\tBatch loss 0.0046\n",
      "| Epoch [ 4/ 5] Iter [691/396]\tBatch loss 0.0046\n",
      "| Epoch [ 4/ 5] Iter [701/396]\tBatch loss 0.0046\n",
      "| Epoch [ 4/ 5] Iter [711/396]\tBatch loss 0.0046\n",
      "| Epoch [ 4/ 5] Iter [721/396]\tBatch loss 0.0046\n",
      "| Epoch [ 4/ 5] Iter [731/396]\tBatch loss 0.0046\n",
      "| Epoch [ 4/ 5] Iter [741/396]\tBatch loss 0.0046\n",
      "| Epoch [ 4/ 5] Iter [751/396]\tBatch loss 0.0045\n",
      "| Epoch [ 4/ 5] Iter [761/396]\tBatch loss 0.0045\n",
      "| Epoch [ 4/ 5] Iter [771/396]\tBatch loss 0.0045\n",
      "| Epoch [ 4/ 5] Iter [781/396]\tBatch loss 0.0045\n",
      "| Epoch [ 4/ 5] Iter [791/396]\tBatch loss 0.0045\n",
      "\n",
      "| Training loss 0.3284\n",
      "Epoch 4/4\n",
      "----------\n",
      "| Epoch [ 5/ 5] Iter [  1/396]\tBatch loss 0.0045\n",
      "| Epoch [ 5/ 5] Iter [ 11/396]\tBatch loss 0.0045\n",
      "| Epoch [ 5/ 5] Iter [ 21/396]\tBatch loss 0.0045\n",
      "| Epoch [ 5/ 5] Iter [ 31/396]\tBatch loss 0.0045\n",
      "| Epoch [ 5/ 5] Iter [ 41/396]\tBatch loss 0.0045\n",
      "| Epoch [ 5/ 5] Iter [ 51/396]\tBatch loss 0.0044\n",
      "| Epoch [ 5/ 5] Iter [ 61/396]\tBatch loss 0.0044\n",
      "| Epoch [ 5/ 5] Iter [ 71/396]\tBatch loss 0.0044\n",
      "| Epoch [ 5/ 5] Iter [ 81/396]\tBatch loss 0.0044\n",
      "| Epoch [ 5/ 5] Iter [ 91/396]\tBatch loss 0.0044\n",
      "| Epoch [ 5/ 5] Iter [101/396]\tBatch loss 0.0044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch [ 5/ 5] Iter [111/396]\tBatch loss 0.0044\n",
      "| Epoch [ 5/ 5] Iter [121/396]\tBatch loss 0.0044\n",
      "| Epoch [ 5/ 5] Iter [131/396]\tBatch loss 0.0043\n",
      "| Epoch [ 5/ 5] Iter [141/396]\tBatch loss 0.0043\n",
      "| Epoch [ 5/ 5] Iter [151/396]\tBatch loss 0.0043\n",
      "| Epoch [ 5/ 5] Iter [161/396]\tBatch loss 0.0043\n",
      "| Epoch [ 5/ 5] Iter [171/396]\tBatch loss 0.0043\n",
      "| Epoch [ 5/ 5] Iter [181/396]\tBatch loss 0.0043\n",
      "| Epoch [ 5/ 5] Iter [191/396]\tBatch loss 0.0043\n",
      "| Epoch [ 5/ 5] Iter [201/396]\tBatch loss 0.0043\n",
      "| Epoch [ 5/ 5] Iter [211/396]\tBatch loss 0.0043\n",
      "| Epoch [ 5/ 5] Iter [221/396]\tBatch loss 0.0043\n",
      "| Epoch [ 5/ 5] Iter [231/396]\tBatch loss 0.0043\n",
      "| Epoch [ 5/ 5] Iter [241/396]\tBatch loss 0.0042\n",
      "| Epoch [ 5/ 5] Iter [251/396]\tBatch loss 0.0042\n",
      "| Epoch [ 5/ 5] Iter [261/396]\tBatch loss 0.0042\n",
      "| Epoch [ 5/ 5] Iter [271/396]\tBatch loss 0.0042\n",
      "| Epoch [ 5/ 5] Iter [281/396]\tBatch loss 0.0042\n",
      "| Epoch [ 5/ 5] Iter [291/396]\tBatch loss 0.0042\n",
      "| Epoch [ 5/ 5] Iter [301/396]\tBatch loss 0.0042\n",
      "| Epoch [ 5/ 5] Iter [311/396]\tBatch loss 0.0042\n",
      "| Epoch [ 5/ 5] Iter [321/396]\tBatch loss 0.0042\n",
      "| Epoch [ 5/ 5] Iter [331/396]\tBatch loss 0.0042\n",
      "| Epoch [ 5/ 5] Iter [341/396]\tBatch loss 0.0041\n",
      "| Epoch [ 5/ 5] Iter [351/396]\tBatch loss 0.0041\n",
      "| Epoch [ 5/ 5] Iter [361/396]\tBatch loss 0.0041\n",
      "| Epoch [ 5/ 5] Iter [371/396]\tBatch loss 0.0041\n",
      "| Epoch [ 5/ 5] Iter [381/396]\tBatch loss 0.0041\n",
      "| Epoch [ 5/ 5] Iter [391/396]\tBatch loss 0.0041\n",
      "| Epoch [ 5/ 5] Iter [401/396]\tBatch loss 0.0041\n",
      "| Epoch [ 5/ 5] Iter [411/396]\tBatch loss 0.0041\n",
      "| Epoch [ 5/ 5] Iter [421/396]\tBatch loss 0.0041\n",
      "| Epoch [ 5/ 5] Iter [431/396]\tBatch loss 0.0041\n",
      "| Epoch [ 5/ 5] Iter [441/396]\tBatch loss 0.0040\n",
      "| Epoch [ 5/ 5] Iter [451/396]\tBatch loss 0.0040\n",
      "| Epoch [ 5/ 5] Iter [461/396]\tBatch loss 0.0040\n",
      "| Epoch [ 5/ 5] Iter [471/396]\tBatch loss 0.0040\n",
      "| Epoch [ 5/ 5] Iter [481/396]\tBatch loss 0.0040\n",
      "| Epoch [ 5/ 5] Iter [491/396]\tBatch loss 0.0040\n",
      "| Epoch [ 5/ 5] Iter [501/396]\tBatch loss 0.0040\n",
      "| Epoch [ 5/ 5] Iter [511/396]\tBatch loss 0.0040\n",
      "| Epoch [ 5/ 5] Iter [521/396]\tBatch loss 0.0040\n",
      "| Epoch [ 5/ 5] Iter [531/396]\tBatch loss 0.0040\n",
      "| Epoch [ 5/ 5] Iter [541/396]\tBatch loss 0.0039\n",
      "| Epoch [ 5/ 5] Iter [551/396]\tBatch loss 0.0039\n",
      "| Epoch [ 5/ 5] Iter [561/396]\tBatch loss 0.0039\n",
      "| Epoch [ 5/ 5] Iter [571/396]\tBatch loss 0.0039\n",
      "| Epoch [ 5/ 5] Iter [581/396]\tBatch loss 0.0039\n",
      "| Epoch [ 5/ 5] Iter [591/396]\tBatch loss 0.0039\n",
      "| Epoch [ 5/ 5] Iter [601/396]\tBatch loss 0.0039\n",
      "| Epoch [ 5/ 5] Iter [611/396]\tBatch loss 0.0039\n",
      "| Epoch [ 5/ 5] Iter [621/396]\tBatch loss 0.0039\n",
      "| Epoch [ 5/ 5] Iter [631/396]\tBatch loss 0.0039\n",
      "| Epoch [ 5/ 5] Iter [641/396]\tBatch loss 0.0039\n",
      "| Epoch [ 5/ 5] Iter [651/396]\tBatch loss 0.0038\n",
      "| Epoch [ 5/ 5] Iter [661/396]\tBatch loss 0.0038\n",
      "| Epoch [ 5/ 5] Iter [671/396]\tBatch loss 0.0038\n",
      "| Epoch [ 5/ 5] Iter [681/396]\tBatch loss 0.0038\n",
      "| Epoch [ 5/ 5] Iter [691/396]\tBatch loss 0.0038\n",
      "| Epoch [ 5/ 5] Iter [701/396]\tBatch loss 0.0038\n",
      "| Epoch [ 5/ 5] Iter [711/396]\tBatch loss 0.0038\n",
      "| Epoch [ 5/ 5] Iter [721/396]\tBatch loss 0.0038\n",
      "| Epoch [ 5/ 5] Iter [731/396]\tBatch loss 0.0038\n",
      "| Epoch [ 5/ 5] Iter [741/396]\tBatch loss 0.0038\n",
      "| Epoch [ 5/ 5] Iter [751/396]\tBatch loss 0.0038\n",
      "| Epoch [ 5/ 5] Iter [761/396]\tBatch loss 0.0037\n",
      "| Epoch [ 5/ 5] Iter [771/396]\tBatch loss 0.0037\n",
      "| Epoch [ 5/ 5] Iter [781/396]\tBatch loss 0.0037\n",
      "| Epoch [ 5/ 5] Iter [791/396]\tBatch loss 0.0037\n",
      "\n",
      "| Training loss 0.2698\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "    \n",
    "    \n",
    "    running_loss, running_corrects, tot = 0.0, 0.0, 0.0\n",
    "    ########################\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    ## Training \n",
    "    for batch_idx, (inputs, labels) in enumerate(dset_loaders):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        running_loss += loss*inputs.shape[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ############################################\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        _, tmplabel = torch.max(labels.data, 1)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += preds.eq(tmplabel).cpu().sum()\n",
    "        tot += labels.size(0)\n",
    "        sys.stdout.write('\\r')\n",
    "        try:\n",
    "            batch_loss = loss.item()\n",
    "        except NameError:\n",
    "            batch_loss = 0\n",
    "\n",
    "        top1error = 1 - float(running_corrects)/tot\n",
    "        if batch_idx % 10 == 0:\n",
    "            sys.stdout.write('| Epoch [%2d/%2d] Iter [%3d/%3d]\\tBatch loss %.4f\\n'\n",
    "                             % (epoch + 1, num_epochs, batch_idx + 1,\n",
    "                            (len(os.listdir('./input/train')) // batch_size), batch_loss/batch_size))\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('\\r')\n",
    "        \n",
    "    #accuracy = float(running_corrects)/N_train\n",
    "    epoch_loss = running_loss/N_train\n",
    "\n",
    "    print('\\n| Training loss %.4f'\\\n",
    "            % (epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [34:05<00:00,  7.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 5005)\n"
     ]
    }
   ],
   "source": [
    "#Evaluate and predict test data\n",
    "sub = pd.read_csv('./input/sample_submission.csv')\n",
    "test_transforms = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "test_set = WhaleDataset(\n",
    "    datafolder='./input/test/', \n",
    "    datatype='test', \n",
    "    transform=test_transforms\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, num_workers=0, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "for (inputs, labels, name) in tqdm(test_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    output = model(inputs)\n",
    "    output = output.cpu().detach().numpy()\n",
    "    for i, (e, n) in enumerate(list(zip(output, name))):\n",
    "        sub.loc[sub['Image'] == n, 'Id'] = ' '.join(label_encoder.inverse_transform(e.argsort()[-5:][::-1]))\n",
    "print(output.shape)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
