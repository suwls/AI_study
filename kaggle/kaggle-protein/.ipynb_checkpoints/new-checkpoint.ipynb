{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pathlib\n",
    "import torch.utils.data\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 666\n",
    "cuda = torch.device('cuda')\n",
    "\n",
    "LABEL_MAP = {\n",
    "0: \"Nucleoplasm\" ,\n",
    "1: \"Nuclear membrane\"   ,\n",
    "2: \"Nucleoli\"   ,\n",
    "3: \"Nucleoli fibrillar center\",   \n",
    "4: \"Nuclear speckles\"   ,\n",
    "5: \"Nuclear bodies\"   ,\n",
    "6: \"Endoplasmic reticulum\"   ,\n",
    "7: \"Golgi apparatus\"  ,\n",
    "8: \"Peroxisomes\"   ,\n",
    "9:  \"Endosomes\"   ,\n",
    "10: \"Lysosomes\"   ,\n",
    "11: \"Intermediate filaments\"  , \n",
    "12: \"Actin filaments\"   ,\n",
    "13: \"Focal adhesion sites\"  ,\n",
    "14: \"Microtubules\"   ,\n",
    "15: \"Microtubule ends\"   ,\n",
    "16: \"Cytokinetic bridge\"   ,\n",
    "17: \"Mitotic spindle\"  ,\n",
    "18: \"Microtubule organizing center\",  \n",
    "19: \"Centrosome\",\n",
    "20: \"Lipid droplets\"   ,\n",
    "21: \"Plasma membrane\"  ,\n",
    "22: \"Cell junctions\"   ,\n",
    "23: \"Mitochondria\"   ,\n",
    "24: \"Aggresome\"   ,\n",
    "25: \"Cytosol\" ,\n",
    "26: \"Cytoplasmic bodies\",\n",
    "27: \"Rods & rings\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtienDataset(Dataset):\n",
    "    BANDS_NAMES = ['_red.png','_green.png','_blue.png','_yellow.png']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_df)\n",
    "    \n",
    "    def __init__(self, images_df, \n",
    "                 base_path, \n",
    "                 image_transform, \n",
    "                 augmentator=None,\n",
    "                 train_mode=True    \n",
    "                ):\n",
    "        if not isinstance(base_path, pathlib.Path):\n",
    "            base_path = pathlib.Path(base_path)\n",
    "            \n",
    "        self.images_df = images_df.copy()\n",
    "        self.image_transform = image_transform\n",
    "        self.augmentator = augmentator\n",
    "        self.images_df.Id = self.images_df.Id.apply(lambda x: base_path / x)\n",
    "        self.mlb = MultiLabelBinarizer(classes=list(LABEL_MAP.keys()))\n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        y = None\n",
    "        X = self._load_multiband_image(index)\n",
    "        if self.train_mode:\n",
    "            y = self._load_multilabel_target(index)\n",
    "        \n",
    "        # augmentator can be for instance imgaug augmentation object\n",
    "        if self.augmentator is not None:\n",
    "            X = self.augmentator(X)\n",
    "            \n",
    "        X = self.image_transform(X)\n",
    "            \n",
    "        return X, y \n",
    "        \n",
    "    def _load_multiband_image(self, index):\n",
    "        row = self.images_df.iloc[index]\n",
    "        image_bands = []\n",
    "        for band_name in self.BANDS_NAMES:\n",
    "            p = str(row.Id.absolute()) + band_name\n",
    "            pil_channel = PIL.Image.open(p)\n",
    "            image_bands.append(pil_channel)\n",
    "            \n",
    "        # lets pretend its a RBGA image to support 4 channels\n",
    "        band4image = PIL.Image.merge('RGBA', bands=image_bands)\n",
    "        return band4image\n",
    "    \n",
    "    def _load_multilabel_target(self, index):\n",
    "        return list(map(int, self.images_df.iloc[index].Target.split(' ')))\n",
    "    \n",
    "        \n",
    "    def collate_func(self, batch):\n",
    "        labels = None\n",
    "        images = [x[0] for x in batch]\n",
    "        \n",
    "        if self.train_mode:\n",
    "            labels = [x[1] for x in batch]\n",
    "            labels_one_hot  = self.mlb.fit_transform(labels)\n",
    "            labels = torch.FloatTensor(labels_one_hot)\n",
    "            \n",
    "        \n",
    "        return torch.stack(images)[:,:4,:,:], labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 셋팅\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.C1 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3,padding=1)\n",
    "        self.C2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3,padding=1)\n",
    "        \n",
    "        self.C3 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3,padding=1)\n",
    "        self.C4 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3,padding=1)\n",
    "        \n",
    "        self.C5 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3,padding=1)\n",
    "        self.C6 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,padding=1)\n",
    "        self.C7 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,padding=1)\n",
    "        self.C8 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,padding=1)\n",
    "        \n",
    "        self.C9 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.C10 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.C11 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.C12 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "\n",
    "        self.L1 = nn.Linear(16*16*64, 512)\n",
    "        self.L2 = nn.Linear(512, 28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.C1(x)\n",
    "        x=self.C2(x)\n",
    "        x=F.max_pool2d(F.relu(x),2)\n",
    "\n",
    "        x=self.C3(x)\n",
    "        x=self.C4(x)\n",
    "        x=F.max_pool2d(F.relu(x),2)\n",
    "        \n",
    "        x=self.C5(x)\n",
    "        x=self.C6(x)\n",
    "        x=self.C7(x)\n",
    "        x=self.C8(x)\n",
    "        x=F.max_pool2d(F.relu(x),2)\n",
    "        \n",
    "        x=self.C9(x)\n",
    "        x=self.C10(x)\n",
    "        x=self.C11(x)\n",
    "        x=self.C12(x)\n",
    "        x=F.max_pool2d(F.relu(x),2)\n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "\n",
    "        x = F.relu(self.L1(x))\n",
    "        x = F.relu(self.L2(x))\n",
    "        \n",
    "#         x=F.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submission\n",
    "def predict_submission(model, submission_load):\n",
    "    all_preds = []\n",
    "#     model.eval()\n",
    "    for i, b in enumerate(submission_load):\n",
    "        if i % 100: print('processing batch {}/{}'.format(i, len(submission_load)))\n",
    "        X, _ = b\n",
    "#         if torch.cuda.is_available():\n",
    "#             X = X.cuda()\n",
    "        pred = model(X)\n",
    "        all_preds.append(pred.sigmoid().cpu().data.numpy())\n",
    "    return np.concatenate(all_preds)\n",
    "        \n",
    "         \n",
    "def make_submission_file(sample_submission_df, predictions):\n",
    "    submissions = []\n",
    "    for row in predictions:\n",
    "        subrow = ' '.join(list([str(i) for i in np.nonzero(row)[0]]))\n",
    "        submissions.append(subrow)\n",
    "    \n",
    "    sample_submission_df['Predicted'] = submissions\n",
    "    sample_submission_df.to_csv('submission.csv', index=None)\n",
    "    \n",
    "    return sample_submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_IMAGES = './input/train/'\n",
    "PATH_TO_TEST_IMAGES = './input/test/'\n",
    "PATH_TO_META = './input/train.csv'\n",
    "SAMPLE_SUBMI = './input/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666\n",
    "DEV_MODE = True\n",
    "    \n",
    "df = pd.read_csv(PATH_TO_META)\n",
    "df_train, df_test  = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "df_submission = pd.read_csv(SAMPLE_SUBMI)\n",
    "\n",
    "# if DEV_MODE:\n",
    "#     df_train = df_train[:200]\n",
    "#     df_test = df_test[:50]\n",
    "#     df_submission = df_submission[:50]\n",
    "\n",
    "# image_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "           transforms.Resize(256),\n",
    "           transforms.ToTensor(),\n",
    "       ])\n",
    "\n",
    " \n",
    "# Prepare datasets and loaders\n",
    "   \n",
    "gtrain = ProteinDataset(df_train, base_path=PATH_TO_IMAGES, image_transform=image_transform)\n",
    "gtest = ProteinDataset(df_test, base_path=PATH_TO_IMAGES, image_transform=image_transform)\n",
    "gsub = ProteinDataset(df_submission, base_path=PATH_TO_TEST_IMAGES, train_mode=False, image_transform=image_transform)\n",
    "\n",
    "train_load = DataLoader(gtrain, collate_fn=gtrain.collate_func, batch_size=256, num_workers=6)\n",
    "test_load = DataLoader(gtest, collate_fn=gtest.collate_func, batch_size=256, num_workers=6)\n",
    "submission_load = DataLoader(gsub, collate_fn=gsub.collate_func, batch_size=256, num_workers=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "torch.Size([8, 4, 3, 3])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "Train Epoch: 1 [0/24857 (0%)]\tLoss: 3.327232\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([25, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "Train Epoch: 2 [0/24857 (0%)]\tLoss: 2.797592\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([25, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "Train Epoch: 3 [0/24857 (0%)]\tLoss: 2.588458\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([25, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "Train Epoch: 4 [0/24857 (0%)]\tLoss: 2.541493\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([256, 4, 256, 256])\n",
      "torch.Size([25, 4, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(),lr = 0.01, momentum = 0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.99))\n",
    "\n",
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx,(data,target) in enumerate(train_load):\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        target = target.long()\n",
    "#         print(target.type())\n",
    "#         print(target)\n",
    "        print(data.size())\n",
    "\n",
    "        output = model(data)\n",
    "#         print(output)\n",
    "# \n",
    "        optimizer.zero_grad()\n",
    "#         loss = criterion(output,target)\n",
    "        loss = criterion(output, torch.max(target, 1)[1])\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#ㅇㄹㄴㅇㄹㅇ\n",
    "        if batch_idx%100==0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                                                                           epoch, batch_idx * len(data), len(train_load.dataset),100. * batch_idx / len(train_load), loss))\n",
    "\n",
    "def find(data):\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    output = model(data)\n",
    "    return output\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing batch 1/46\n",
      "processing batch 2/46\n",
      "processing batch 3/46\n",
      "processing batch 4/46\n",
      "processing batch 5/46\n",
      "processing batch 6/46\n",
      "processing batch 7/46\n",
      "processing batch 8/46\n",
      "processing batch 9/46\n",
      "processing batch 10/46\n",
      "processing batch 11/46\n",
      "processing batch 12/46\n",
      "processing batch 13/46\n",
      "processing batch 14/46\n",
      "processing batch 15/46\n",
      "processing batch 16/46\n",
      "processing batch 17/46\n",
      "processing batch 18/46\n",
      "processing batch 19/46\n",
      "processing batch 20/46\n",
      "processing batch 21/46\n",
      "processing batch 22/46\n",
      "processing batch 23/46\n",
      "processing batch 24/46\n",
      "processing batch 25/46\n",
      "processing batch 26/46\n",
      "processing batch 27/46\n",
      "processing batch 28/46\n",
      "processing batch 29/46\n",
      "processing batch 30/46\n",
      "processing batch 31/46\n",
      "processing batch 32/46\n",
      "processing batch 33/46\n",
      "processing batch 34/46\n",
      "processing batch 35/46\n",
      "processing batch 36/46\n",
      "processing batch 37/46\n",
      "processing batch 38/46\n",
      "processing batch 39/46\n",
      "processing batch 40/46\n",
      "processing batch 41/46\n",
      "processing batch 42/46\n",
      "processing batch 43/46\n",
      "processing batch 44/46\n",
      "processing batch 45/46\n"
     ]
    }
   ],
   "source": [
    "submission_predictions =predict_submission(model, submission_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.5        0.9006689  ... 0.5840846  0.5        0.5       ]\n",
      " [0.5        0.5        0.79495066 ... 0.933582   0.5        0.5       ]\n",
      " [0.5        0.5        0.5        ... 0.9072634  0.5        0.5       ]\n",
      " ...\n",
      " [0.5        0.5        0.7778057  ... 0.900845   0.5        0.5       ]\n",
      " [0.5        0.5        0.8303375  ... 0.7795607  0.5        0.5       ]\n",
      " [0.5        0.5        0.68358487 ... 0.96920514 0.5        0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# prepare the submission file and \n",
    "THRESHOLD = 0.8\n",
    "print(submission_predictions)\n",
    "p = submission_predictions>THRESHOLD\n",
    "\n",
    "submission_file = make_submission_file(sample_submission_df=df_submission,predictions=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008af0-bad0-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>2 4 5 7 19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000a892-bacf-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>2 5 7 19 21 23 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006faa6-bac7-11e8-b2b7-ac1f6b6435d0</td>\n",
       "      <td>6 21 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0008baca-bad7-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>2 5 7 19 21 23 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000cce7e-bad4-11e8-b2b8-ac1f6b6435d0</td>\n",
       "      <td>6 7 19 21 23 25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id          Predicted\n",
       "0  00008af0-bad0-11e8-b2b8-ac1f6b6435d0         2 4 5 7 19\n",
       "1  0000a892-bacf-11e8-b2b8-ac1f6b6435d0  2 5 7 19 21 23 25\n",
       "2  0006faa6-bac7-11e8-b2b7-ac1f6b6435d0            6 21 25\n",
       "3  0008baca-bad7-11e8-b2b9-ac1f6b6435d0  2 5 7 19 21 23 25\n",
       "4  000cce7e-bad4-11e8-b2b8-ac1f6b6435d0    6 7 19 21 23 25"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
